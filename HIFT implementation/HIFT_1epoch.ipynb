{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9NQzKYfk12HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3b5432-1f73-4be6-a2b0-b70b4439436a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['Getting started.pdf', 'My account info.zip', 'final revision.gdoc', 'Untitled spreadsheet.gsheet', 'Drivez', 'المقرر حتى 15-3.docx 3rd Sec..pdf', 'Untitled presentation.gslides', 'dynamic  (Revision).pdf', 'Untitled project.gscript', 'Cpp All in One for Dummies - FreePdf-Books.com.pdf', 'Untitled Jam.gjam', 'Abdelrahman.pdf', 'Cutting tool materials and economics of cutting processes.gdoc', 'حق الجنسية (1).docx', 'حق الجنسية.docx', 'Reading Skills.pdf', 'Ohms Law verification (1).pdf', 'Lab Safety.pdf', 'Ohms Law verification.pdf', 'Kirchhof.pdf', 'Superposition and thelenin.pdf', 'circuits 2101645.pdf', 'Writting skills.pdf', 'Section 8', 'AC 1.pdf', 'Notes', 'AC 2.pdf', 'Math 2 (Fall 2021 3rd seamster) ', 'circuits.pdf', 'Lec 07 - Civil Engineering.pdf', 'Lec 08 - Mechanical Engineering.pdf', 'Lec 09 - Electrical Engineering.pdf', '[Witanime.com] JK EP 02 BD-FHD.mp4', 'I Want to thank you for every moment I spend with yo1.pdf', 'I Want to thank you for every moment I spend with yo2.pdf', 'Electromagnetic Fields_Ch (3 )_Electric Potentail and Energy - Fall 2021.pdf', 'IMG_1541.JPG', 'Lab Safety2.pdf', 'sheet 1.pdf', 'sheet 2.pdf', 'sheet 3.pdf', '111.pdf', 'CV (2).pdf', 'exp5.pdf', 'exp 6.pdf', 'exp 7.pdf', 'lab7.pdf', 'exp 8 (1).pdf', 'exp 8.pdf', 'exp7.pdf', 'lab 8.pdf', 'exp 11.pdf', 'exp12 (1).pdf', 'exp 10_.pdf', 'exp 9.pdf', 'exp12.pdf', 'measure4.pdf', 'Abdelrahman Atef Saad.gdoc', 'Excel', '8C8B8488-BF2D-40EB-94BC-FE30420B45E9.png', 'materials report 1.pdf', 'Valorant', 'Copy of Midterm Fall 2021 Solution (Steps).pdf', 'Copy of Exercise 4.pdf', 'Aim Benchmarks.gsheet', 'Egypt-High-School-Grade-2023-Mohamedovic.com.xlsx', 'CV.pdf', 'Accounts', 'Classroom', 'Untitled document (1).gdoc', 'Screenshot_2023-10-11-02-16-57-78.png', 'S_K_LW_JQ_RM', 'LW_JQ_R', 'S_LW_R', 'K_LW_R', 'S_LW', 'dps_sup', '254', 'S_K_LW_JQ', 'S_K (1)', 'S_JQ', 'K', 'JQ', 'R', 'LW', '253', '255', 'oout.xlsx', 'safeum.xlsx', '15.xlsx', 'Colab Notebooks', 'R_JQ', 'S_K', 'Discrete Mathematics', 'SecuremessengerSafeUM_1.1.0.1548_Apkpure_source_from_JADX_commentated.zip', 'Account-Manager-Discord-master.zip', \"What to Watch from Prof. Ayman Bahaa's Playlist.gdoc\", '1.zip', 'Lethal.Company.v40-OFME.rar', 'LethalCompany_Fix_Repair_Steam_Generic.rar', 'control.pdf', 'Copy of manet.mp4', 'Backup', 'MT Degrees.xlsx', 'DSProject_notFinal.docx', 'cont2.pdf', 'lab1,2.pdf', 'lab3,4.pdf', 'lab5,6.pdf', 'lab7,8.pdf', 'lab9,10.pdf', 'lab11,12.pdf', 'DS-Project', 'acc.xlsx', 'Duck_2.mp4', 'Old vids', 'proposal (2).docx', 'proposal_draft1.docx', 'vids', 'pone.0132175.pdf', 'rahman-et-al-2022-assessment-of-antioxidant-activity-and-physicochemical-properties-of-polymeric-bio-composites-using.pdf', 'CV (1).pdf', 'Copy of projects.gsheet', 'PaaS.gdoc', 'كلية هندسة البترول.docx', 'CamScanner 07-03-2024 01.36 (1).pdf', 'CamScanner 07-03-2024 01.36.pdf', 'نموذج إعلان ٢٠٢٤.docx', 'نموذج إعلان ٢٠٢٤.docx.pdf', 'grad projects.gsheet', 'Untitled document.gdoc', 'ML Notes.gdoc', 'docker task.gdoc', 'ppt3CF6.pptm [Autosaved]1 (1).pptm', 'ppt3CF6.pptm [Autosaved]1.pptm', 'ppt3CF6.pptm [Autosaved]1.pptm.gslides', 'ppt3CF6.pptm [Autosaved]1.pptm.pptx', 'Copy of Quiz1B_v1_Answer.docx', 'abdelrahman_cv.pdf', 'cv_1.docx', 'cv_1.gdoc', 'cv_1.pdf', 'IMG_9083.JPG', 'IMG_9037.JPG', 'move_it_rig_move_safety_for_roughnec-tD-kmXI2M1c_fmt43.mp4', 'Drilling and Workover Safety copy.pptx', 'Safety and Health for Engineers (Second Edition).pdf', '.ipynb_checkpoints', 'full_data.zip', 'val_data.zip']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# List the contents of your shared drives\n",
        "print(os.listdir('/content/drive/MyDrive'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/full_data.zip'\n",
        "extract_path = '/content/full_data/'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "toxGiI-BJLcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "45613626-d267-4227-8bd4-78e60fe9c530"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 28] No space left on device",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-779c35550780>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extraction complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1712\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1713\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1714\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    747\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    748\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/val_data.zip'\n",
        "extract_path = '/content/full_data/validation'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "nOiaxbjeakab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class GOT10KDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, output_size=(13, 13)):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.output_size = output_size\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for folder in os.listdir(self.root_dir):\n",
        "            folder_path = os.path.join(self.root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                frame_paths = sorted([\n",
        "                    os.path.join(folder_path, img)\n",
        "                    for img in os.listdir(folder_path)\n",
        "                    if img.endswith(('.jpg', '.png'))\n",
        "                ])\n",
        "\n",
        "                # Ensure we have enough frames to form a sequence\n",
        "                if len(frame_paths) >= 2:\n",
        "                    for i in range(len(frame_paths) - 1):\n",
        "                        template_path = frame_paths[i]\n",
        "                        search_path = frame_paths[i + 1]\n",
        "\n",
        "                        # Example classification label with the required output size (H, W)\n",
        "                        cls_label = torch.zeros(2, *self.output_size)  # Assuming 2 classes and binary label maps\n",
        "                        cls_label[0, :, :] = 1  # Set the first channel to ones (example)\n",
        "\n",
        "                        reg_label = torch.tensor([0.5, 0.5, 0.5, 0.5])  # Example regression label\n",
        "                        samples.append((template_path, search_path, cls_label, reg_label))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        template_path, search_path, cls_label, reg_label = self.samples[idx]\n",
        "        template = Image.open(template_path).convert('RGB')\n",
        "        search = Image.open(search_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            template = self.transform(template)\n",
        "            search = self.transform(search)\n",
        "\n",
        "        return template, search, cls_label, reg_label\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = GOT10KDataset(root_dir='/content/full_data/train', transform=data_transform)\n",
        "\n"
      ],
      "metadata": {
        "id": "RkW3ukeIUNah"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "batch_size = 1\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EYGTu05TUWj9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv2d, Linear, Dropout, MultiheadAttention, GroupNorm\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "        self.features = nn.Sequential(*list(alexnet.features.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.features:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "        return outputs[-3:]\n",
        "\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_heads=8):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.conv = Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, out_channels, 1, 1))\n",
        "        self.multihead_attn = MultiheadAttention(embed_dim=out_channels, num_heads=num_heads)\n",
        "        self.norm = GroupNorm(8, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.conv(x1)\n",
        "        x2 = self.conv(x2)\n",
        "        x1 += self.positional_encoding\n",
        "        x2 += self.positional_encoding\n",
        "\n",
        "        B, C, H, W = x1.size()\n",
        "        x1_flat = x1.view(B, C, -1).permute(2, 0, 1)\n",
        "        x2_flat = x2.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(x1_flat, x2_flat, x2_flat)\n",
        "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        output = self.norm(x1 + attn_output)\n",
        "        return output\n",
        "\n",
        "class FeatureDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=8):\n",
        "        super(FeatureDecoder, self).__init__()\n",
        "        self.multihead_attn = MultiheadAttention(embed_dim=in_channels, num_heads=num_heads)\n",
        "        self.norm1 = GroupNorm(8, in_channels)\n",
        "        self.norm2 = GroupNorm(8, in_channels)\n",
        "        self.ffn = nn.Sequential(\n",
        "            Linear(in_channels, in_channels * 4),\n",
        "            nn.ReLU(),\n",
        "            Linear(in_channels * 4, in_channels),\n",
        "        )\n",
        "        self.dropout = Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        x_flat = x.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(x_flat, x_flat, x_flat)\n",
        "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        x = self.norm1(x + attn_output)\n",
        "        x = x + self.ffn(x.view(B, C, -1).permute(2, 0, 1)).permute(1, 2, 0).view(B, C, H, W)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ClassificationAndRegression(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ClassificationAndRegression, self).__init__()\n",
        "        self.cls_conv = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "        self.reg_conv = nn.Conv2d(in_channels, 4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_output = self.cls_conv(x)\n",
        "        reg_output = self.reg_conv(x)\n",
        "        return cls_output, reg_output\n",
        "\n",
        "class ModulationLayer(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super(ModulationLayer, self).__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.gap(x).view(b, c)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class HiFT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HiFT, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.feature_encoder = FeatureEncoder(in_channels=256, out_channels=256)\n",
        "        self.modulation_layer = ModulationLayer(in_channels=256)\n",
        "        self.feature_decoder = FeatureDecoder(in_channels=256)\n",
        "        self.classification_and_regression = ClassificationAndRegression(in_channels=256)\n",
        "        self.concat_conv = nn.Conv2d(256 * 2, 256, kernel_size=1)\n",
        "\n",
        "    def concatenate_and_conv(self, z, x):\n",
        "        concatenated = torch.cat((z, x), dim=1)\n",
        "        fused = self.concat_conv(concatenated)\n",
        "        return fused\n",
        "\n",
        "    def forward(self, z, x):\n",
        "        z_features = self.feature_extractor(z)\n",
        "        x_features = self.feature_extractor(x)\n",
        "\n",
        "        encoded_features = []\n",
        "        for i in range(3):\n",
        "            encoded = self.feature_encoder(z_features[i], x_features[i])\n",
        "            modulated = self.modulation_layer(encoded)\n",
        "            fused_features = self.concatenate_and_conv(z_features[i], x_features[i])\n",
        "            decoded = self.feature_decoder(fused_features)\n",
        "            encoded_features.append(decoded)\n",
        "\n",
        "        final_features = sum(encoded_features)\n",
        "        cls_output, reg_output = self.classification_and_regression(final_features)\n",
        "\n",
        "        return cls_output, reg_output\n"
      ],
      "metadata": {
        "id": "wA-hcEysUWq1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def train(model, dataloader, epochs, device):\n",
        "    model.train()\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_reg = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (template, search, cls_label, reg_label) in enumerate(dataloader):\n",
        "            try:\n",
        "              template, search, cls_label, reg_label = template.to(device), search.to(device), cls_label.to(device), reg_label.to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              cls_output, reg_output = model(template, search)\n",
        "\n",
        "              # Ensure cls_label has the correct shape (e.g., [batch_size, num_classes, H, W])\n",
        "              cls_loss = criterion_cls(cls_output, cls_label)\n",
        "\n",
        "              # Reshape reg_label to match reg_output\n",
        "              reg_label = reg_label.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 13, 13)  # Expand dimensions to match reg_output\n",
        "              reg_loss = criterion_reg(reg_output, reg_label)\n",
        "\n",
        "              loss = cls_loss + reg_loss\n",
        "\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "              running_loss += loss.item()\n",
        "\n",
        "              if i % 10 == 9:\n",
        "                  print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
        "                  running_loss = 0.0\n",
        "                  # early break, the dataset is too big\n",
        "                  if i >= 9999:\n",
        "                    break\n",
        "            except:\n",
        "              break\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model and move to device\n",
        "model = HiFT().to(device)\n",
        "\n",
        "# Instantiate dataset and dataloader\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "train(model, train_dataloader, epochs=1, device=device)"
      ],
      "metadata": {
        "id": "qSdPsUqcUWwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2562af3e-6533-4abb-e678-c8a1c3ee92a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 10] loss: 1.542\n",
            "[1, 20] loss: 0.639\n",
            "[1, 30] loss: 0.457\n",
            "[1, 40] loss: 0.321\n",
            "[1, 50] loss: 0.291\n",
            "[1, 60] loss: 0.279\n",
            "[1, 70] loss: 0.183\n",
            "[1, 80] loss: 0.159\n",
            "[1, 90] loss: 0.134\n",
            "[1, 100] loss: 0.148\n",
            "[1, 110] loss: 0.114\n",
            "[1, 120] loss: 0.117\n",
            "[1, 130] loss: 0.103\n",
            "[1, 140] loss: 0.093\n",
            "[1, 150] loss: 0.088\n",
            "[1, 160] loss: 0.064\n",
            "[1, 170] loss: 0.073\n",
            "[1, 180] loss: 0.075\n",
            "[1, 190] loss: 0.058\n",
            "[1, 200] loss: 0.056\n",
            "[1, 210] loss: 0.057\n",
            "[1, 220] loss: 0.048\n",
            "[1, 230] loss: 0.035\n",
            "[1, 240] loss: 0.044\n",
            "[1, 250] loss: 0.046\n",
            "[1, 260] loss: 0.042\n",
            "[1, 270] loss: 0.046\n",
            "[1, 280] loss: 0.046\n",
            "[1, 290] loss: 0.051\n",
            "[1, 300] loss: 0.051\n",
            "[1, 310] loss: 0.051\n",
            "[1, 320] loss: 0.047\n",
            "[1, 330] loss: 0.032\n",
            "[1, 340] loss: 0.031\n",
            "[1, 350] loss: 0.032\n",
            "[1, 360] loss: 0.038\n",
            "[1, 370] loss: 0.031\n",
            "[1, 380] loss: 0.041\n",
            "[1, 390] loss: 0.032\n",
            "[1, 400] loss: 0.025\n",
            "[1, 410] loss: 0.038\n",
            "[1, 420] loss: 0.033\n",
            "[1, 430] loss: 0.025\n",
            "[1, 440] loss: 0.021\n",
            "[1, 450] loss: 0.026\n",
            "[1, 460] loss: 0.018\n",
            "[1, 470] loss: 0.028\n",
            "[1, 480] loss: 0.022\n",
            "[1, 490] loss: 0.022\n",
            "[1, 500] loss: 0.023\n",
            "[1, 510] loss: 0.020\n",
            "[1, 520] loss: 0.017\n",
            "[1, 530] loss: 0.038\n",
            "[1, 540] loss: 0.024\n",
            "[1, 550] loss: 0.023\n",
            "[1, 560] loss: 0.020\n",
            "[1, 570] loss: 0.021\n",
            "[1, 580] loss: 0.017\n",
            "[1, 590] loss: 0.016\n",
            "[1, 600] loss: 0.013\n",
            "[1, 610] loss: 0.020\n",
            "[1, 620] loss: 0.022\n",
            "[1, 630] loss: 0.015\n",
            "[1, 640] loss: 0.013\n",
            "[1, 650] loss: 0.024\n",
            "[1, 660] loss: 0.018\n",
            "[1, 670] loss: 0.013\n",
            "[1, 680] loss: 0.021\n",
            "[1, 690] loss: 0.020\n",
            "[1, 700] loss: 0.013\n",
            "[1, 710] loss: 0.018\n",
            "[1, 720] loss: 0.017\n",
            "[1, 730] loss: 0.015\n",
            "[1, 740] loss: 0.018\n",
            "[1, 750] loss: 0.021\n",
            "[1, 760] loss: 0.012\n",
            "[1, 770] loss: 0.009\n",
            "[1, 780] loss: 0.017\n",
            "[1, 790] loss: 0.010\n",
            "[1, 800] loss: 0.012\n",
            "[1, 810] loss: 0.018\n",
            "[1, 820] loss: 0.013\n",
            "[1, 830] loss: 0.016\n",
            "[1, 840] loss: 0.008\n",
            "[1, 850] loss: 0.011\n",
            "[1, 860] loss: 0.009\n",
            "[1, 870] loss: 0.009\n",
            "[1, 880] loss: 0.010\n",
            "[1, 890] loss: 0.014\n",
            "[1, 900] loss: 0.014\n",
            "[1, 910] loss: 0.013\n",
            "[1, 920] loss: 0.011\n",
            "[1, 930] loss: 0.011\n",
            "[1, 940] loss: 0.012\n",
            "[1, 950] loss: 0.009\n",
            "[1, 960] loss: 0.008\n",
            "[1, 970] loss: 0.017\n",
            "[1, 980] loss: 0.013\n",
            "[1, 990] loss: 0.008\n",
            "[1, 1000] loss: 0.013\n",
            "[1, 1010] loss: 0.012\n",
            "[1, 1020] loss: 0.008\n",
            "[1, 1030] loss: 0.016\n",
            "[1, 1040] loss: 0.014\n",
            "[1, 1050] loss: 0.012\n",
            "[1, 1060] loss: 0.012\n",
            "[1, 1070] loss: 0.009\n",
            "[1, 1080] loss: 0.007\n",
            "[1, 1090] loss: 0.010\n",
            "[1, 1100] loss: 0.009\n",
            "[1, 1110] loss: 0.011\n",
            "[1, 1120] loss: 0.008\n",
            "[1, 1130] loss: 0.006\n",
            "[1, 1140] loss: 0.011\n",
            "[1, 1150] loss: 0.010\n",
            "[1, 1160] loss: 0.008\n",
            "[1, 1170] loss: 0.009\n",
            "[1, 1180] loss: 0.011\n",
            "[1, 1190] loss: 0.007\n",
            "[1, 1200] loss: 0.010\n",
            "[1, 1210] loss: 0.013\n",
            "[1, 1220] loss: 0.009\n",
            "[1, 1230] loss: 0.008\n",
            "[1, 1240] loss: 0.008\n",
            "[1, 1250] loss: 0.008\n",
            "[1, 1260] loss: 0.010\n",
            "[1, 1270] loss: 0.007\n",
            "[1, 1280] loss: 0.008\n",
            "[1, 1290] loss: 0.010\n",
            "[1, 1300] loss: 0.007\n",
            "[1, 1310] loss: 0.009\n",
            "[1, 1320] loss: 0.007\n",
            "[1, 1330] loss: 0.008\n",
            "[1, 1340] loss: 0.006\n",
            "[1, 1350] loss: 0.007\n",
            "[1, 1360] loss: 0.010\n",
            "[1, 1370] loss: 0.007\n",
            "[1, 1380] loss: 0.007\n",
            "[1, 1390] loss: 0.007\n",
            "[1, 1400] loss: 0.006\n",
            "[1, 1410] loss: 0.006\n",
            "[1, 1420] loss: 0.006\n",
            "[1, 1430] loss: 0.006\n",
            "[1, 1440] loss: 0.007\n",
            "[1, 1450] loss: 0.010\n",
            "[1, 1460] loss: 0.012\n",
            "[1, 1470] loss: 0.006\n",
            "[1, 1480] loss: 0.009\n",
            "[1, 1490] loss: 0.008\n",
            "[1, 1500] loss: 0.006\n",
            "[1, 1510] loss: 0.007\n",
            "[1, 1520] loss: 0.008\n",
            "[1, 1530] loss: 0.010\n",
            "[1, 1540] loss: 0.008\n",
            "[1, 1550] loss: 0.007\n",
            "[1, 1560] loss: 0.005\n",
            "[1, 1570] loss: 0.005\n",
            "[1, 1580] loss: 0.005\n",
            "[1, 1590] loss: 0.008\n",
            "[1, 1600] loss: 0.008\n",
            "[1, 1610] loss: 0.007\n",
            "[1, 1620] loss: 0.006\n",
            "[1, 1630] loss: 0.006\n",
            "[1, 1640] loss: 0.006\n",
            "[1, 1650] loss: 0.006\n",
            "[1, 1660] loss: 0.006\n",
            "[1, 1670] loss: 0.007\n",
            "[1, 1680] loss: 0.007\n",
            "[1, 1690] loss: 0.007\n",
            "[1, 1700] loss: 0.006\n",
            "[1, 1710] loss: 0.007\n",
            "[1, 1720] loss: 0.005\n",
            "[1, 1730] loss: 0.005\n",
            "[1, 1740] loss: 0.005\n",
            "[1, 1750] loss: 0.006\n",
            "[1, 1760] loss: 0.007\n",
            "[1, 1770] loss: 0.007\n",
            "[1, 1780] loss: 0.004\n",
            "[1, 1790] loss: 0.005\n",
            "[1, 1800] loss: 0.007\n",
            "[1, 1810] loss: 0.006\n",
            "[1, 1820] loss: 0.007\n",
            "[1, 1830] loss: 0.006\n",
            "[1, 1840] loss: 0.006\n",
            "[1, 1850] loss: 0.004\n",
            "[1, 1860] loss: 0.005\n",
            "[1, 1870] loss: 0.005\n",
            "[1, 1880] loss: 0.007\n",
            "[1, 1890] loss: 0.008\n",
            "[1, 1900] loss: 0.005\n",
            "[1, 1910] loss: 0.004\n",
            "[1, 1920] loss: 0.007\n",
            "[1, 1930] loss: 0.006\n",
            "[1, 1940] loss: 0.005\n",
            "[1, 1950] loss: 0.007\n",
            "[1, 1960] loss: 0.007\n",
            "[1, 1970] loss: 0.005\n",
            "[1, 1980] loss: 0.004\n",
            "[1, 1990] loss: 0.005\n",
            "[1, 2000] loss: 0.005\n",
            "[1, 2010] loss: 0.003\n",
            "[1, 2020] loss: 0.006\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_cls_loss = 0.0\n",
        "    total_reg_loss = 0.0\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_reg = nn.MSELoss()\n",
        "\n",
        "    for i, (template, search, cls_label, reg_label) in enumerate(dataloader):\n",
        "          try:\n",
        "            template, search, cls_label, reg_label = (\n",
        "                template.to(device),\n",
        "                search.to(device),\n",
        "                cls_label.to(device),\n",
        "                reg_label.to(device),\n",
        "            )\n",
        "            cls_output, reg_output = model(template, search)\n",
        "\n",
        "            # For CrossEntropyLoss, cls_label should be of shape [batch_size, H, W] if cls_output is [batch_size, num_classes, H, W]\n",
        "            cls_label = cls_label.squeeze(-1).squeeze(-1)  # Remove extra dimensions to match output shape requirements\n",
        "\n",
        "            # Compute classification loss\n",
        "            cls_loss = criterion_cls(cls_output, cls_label)\n",
        "\n",
        "            # Adjust reg_label to match the shape of reg_output\n",
        "            reg_label = reg_label.unsqueeze(-1).unsqueeze(-1)  # Add two dimensions: [batch_size, 4, 1, 1]\n",
        "            reg_label = reg_label.expand_as(reg_output)  # Expand to match reg_output shape\n",
        "\n",
        "            # Compute regression loss\n",
        "            reg_loss = criterion_reg(reg_output, reg_label)\n",
        "\n",
        "            # Accumulate total loss\n",
        "            total_cls_loss += cls_loss.item()\n",
        "            total_reg_loss += reg_loss.item()\n",
        "          except:\n",
        "            break\n",
        "\n",
        "    avg_cls_loss = total_cls_loss / i\n",
        "    avg_reg_loss = total_reg_loss / i\n",
        "    avg_total_loss = avg_cls_loss + avg_reg_loss\n",
        "\n",
        "    print(f\"Validation Classification Loss: {avg_cls_loss:.3f}\")\n",
        "    print(f\"Validation Regression Loss: {avg_reg_loss:.3f}\")\n",
        "    print(f\"Validation Total Loss: {avg_total_loss:.3f}\")\n",
        "\n",
        "# Validate the model\n",
        "validate(model, val_dataloader, device)\n"
      ],
      "metadata": {
        "id": "fr7JZklZUW0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee139868-82a3-48a1-f85c-a78dd0921e8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Classification Loss: 0.000\n",
            "Validation Regression Loss: 0.005\n",
            "Validation Total Loss: 0.005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ec-XYkcPN6AJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}