{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9NQzKYfk12HL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69baf36-fa9c-44be-9f9a-b9f0e744b160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "['VID_20210528_174657_178.mp4', 'VID_20210528_174701_440.mp4', 'Infrastructure VS. AD-HOC.pdf.gdrive', 'افكار لكتابه السيره الذاتية.gdoc', 'British Council Certificate.pdf', 'IMG_20211010_131359 (5).jpg', 'IMG_20211010_131359 (4).jpg', 'IMG_20211010_131359 (3).jpg', 'IMG_20211010_131359 (2).jpg', 'IMG_20211010_131359 (1).jpg', 'IMG_20211010_131359.jpg', 'IMG_20211010_192217.jpg', 'Screenshot_2021-10-21-11-18-21-80_40deb401b9ffe8e1df2f1cc5ba480b12.jpg', 'exp1s.pdf', 'EXP2.pdf', 'KIRSSOF.pdf', '16372573386196998407231969063766.jpg', '16372575382426078223689767951657.jpg', '555555.pdf', 'abdelrhman Zain Mohamed 2101646(2).pdf', 'EXP 5.pdf', 'EXP 6.pdf', 'Abdelrhman zain mohamed 2101646(3).pdf', 'نسخة من Sections 7 & 8.pdf', 'exp1sssss.pdf', 'abdelrhman zain 1 .pdf', 'ABDELRHman zain 2 pdf.pdf', 'ABDEL rhman zain . 3.pdf', 'exp measure 4 .pdf', 'Introduction.pdf', 'exp 6.pdf', 'exp 7.pdf', 'exp 8.pdf', 'exp9.pdf', 'exp 10  (1).pdf', 'exp 10 .pdf', 'exp 11 .pdf', 'abdelrhman zain.pptx', 'project montage.mp4', 'exp12.pdf', 'exp 11 correct .pdf', 'measurements sheet 4.pdf', 'abdelrhman zain 3.pdf', 'Course_Certificate_En.pdf', 'كلية.pdf', 'zain.jpeg', 'نسخة من Spring 2021.pdf', 'PicsArt_07-06-02.27.24.jpg', 'download-pdf-ebooks.org-1470055289-363.pdf', 'zain info.pdf', 'CS50x (1).pdf', 'Abdelrhman zain mohamed ahmed', '572638bc-18f0-4ec9-aefb-ba20c3f479c3.pdf', 'Screenshot_2022-09-24-16-19-54-35_40deb401b9ffe8e1df2f1cc5ba480b12.jpg', 'my new cv.pdf', 'تحديد معاد سكشن (fundamental (section 8,33,34,35.gform', 'IMG_20221103_121122.jpg', 'NEW_PCB_2023-01-30.pcbdoc', 'PROJECT', 'نسخة من Day 16 part 2.mkv', 'نسخة من (Spring 2021) Midterm Exam.pdf', 'WhatsApp Image 2023-04-13 at 14.26.32.jpg', 'WhatsApp Image 2023-04-13 at 14.27.07.jpg', 'WhatsApp Image 2023-04-13 at 14.27.16.jpg', 'my  cv.pdf', 'Copy of [6] Mosfet and Gates.pdf', 'university id.pdf', 'Emailing my  cv.pdf', 'نموذج بدون عنوان (1).gform', 'NAID', 'CamScanner 08-26-2023 15.06_1.jpg', 'Abdelrahman Zain Mohamed Ahmed.pdf', 'نسخة من CamScanner 08-26-2023 15.06_1.jpg', 'Python_Essentials_1_Badge20230908-28-5nzv7a.pdf', 'نموذج بدون عنوان.gform', \"Abdelrhman Zain's CV (1).pdf\", 'Python_Essentials_2_Badge20230922-30-rpmyck.pdf', \"Abdelrhman zain'CV.pdf\", 'Coursera KMQ3U8YBAT29.pdf', 'Classroom', 'control assignment.pdf', 'CamScanner 12-06-2023 18.29_1.jpg', 'my_data.pdf', 'data intern.pdf', 'التعهد.pdf', 'نسخة من 2013.pdf', 'Lab 3&4 .pdf', 'Lab 1&2.pdf', 'Lab 5&6.pdf', 'lab 7 & 8.pdf', 'Lab 9&10.pdf', 'Lab 11&12 .pdf', 'assignment 2 control.pdf', 'assignment 3,4.pdf', \"Abdelrhman Zain's CV.pdf\", 'الاقرار.pdf', 'Emailing lab3_algo_zain.pdf', 'IMG_20240603_124354.jpg', 'abdelrhman zain(1) (4).pdf', 'abdelrhman zain(1) (3).pdf', 'abdelrhman zain(1) (2).pdf', 'abdelrhman zain(1) (1).pdf', 'abdelrhman zain(1).pdf', 'Abdelrhman Zain_CV_For_WideBot (1).pdf', 'Abdelrhman Zain_CV_For_Ejada.pdf', 'Abdelrhman Zain_CV_For_WideBot.pdf', 'اقرار.pdf', 'Abdelrhman_Zain.pdf', 'YouCut_20240723_233854680.mp4', 'Abdelrhman Zain.pdf', 'Google AI Studio', 'Teams CVs', 'Abdelrhman Zain Mohamed Ahmed.pdf', 'DL-ASU_Task_00', 'Colab Notebooks', 'Teeth_Dataset', 'نسخة من UDACITY Introduction to Generative AI with AWS Project Documentation Report.gdoc', 'full_data.zip']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "# List the contents of your shared drives\n",
        "print(os.listdir('/content/drive/MyDrive'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/full_data.zip'\n",
        "extract_path = '/content/full_data/'\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction complete.\")"
      ],
      "metadata": {
        "id": "toxGiI-BJLcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GOT10KDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = self._load_samples()\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for folder in os.listdir(self.root_dir):\n",
        "            folder_path = os.path.join(self.root_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                template_path = os.path.join(folder_path, 'template.jpg')\n",
        "                search_path = os.path.join(folder_path, 'search.jpg')\n",
        "                cls_label = torch.tensor([0, 1])  # Example classification label\n",
        "                reg_label = torch.tensor([0.5, 0.5, 0.5, 0.5])  # Example regression label\n",
        "                samples.append((template_path, search_path, cls_label, reg_label))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        template_path, search_path, cls_label, reg_label = self.samples[idx]\n",
        "        template = Image.open(template_path).convert('RGB')\n",
        "        search = Image.open(search_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            template = self.transform(template)\n",
        "            search = self.transform(search)\n",
        "\n",
        "        return template, search, cls_label, reg_label\n",
        "\n",
        "# Example of using the dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = GOT10KDataset(root_dir='/content/full_data', transform=data_transform)\n"
      ],
      "metadata": {
        "id": "RkW3ukeIUNah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "batch_size = 4\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "EYGTu05TUWj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv2d, Linear, Dropout, MultiheadAttention, GroupNorm\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "        self.features = nn.Sequential(*list(alexnet.features.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for layer in self.features:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "        return outputs[-3:]\n",
        "\n",
        "class FeatureEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_heads=8):\n",
        "        super(FeatureEncoder, self).__init__()\n",
        "        self.conv = Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, out_channels, 1, 1))\n",
        "        self.multihead_attn = MultiheadAttention(embed_dim=out_channels, num_heads=num_heads)\n",
        "        self.norm = GroupNorm(8, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.conv(x1)\n",
        "        x2 = self.conv(x2)\n",
        "        x1 += self.positional_encoding\n",
        "        x2 += self.positional_encoding\n",
        "\n",
        "        B, C, H, W = x1.size()\n",
        "        x1_flat = x1.view(B, C, -1).permute(2, 0, 1)\n",
        "        x2_flat = x2.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(x1_flat, x2_flat, x2_flat)\n",
        "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        output = self.norm(x1 + attn_output)\n",
        "        return output\n",
        "\n",
        "class FeatureDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=8):\n",
        "        super(FeatureDecoder, self).__init__()\n",
        "        self.multihead_attn = MultiheadAttention(embed_dim=in_channels, num_heads=num_heads)\n",
        "        self.norm1 = GroupNorm(8, in_channels)\n",
        "        self.norm2 = GroupNorm(8, in_channels)\n",
        "        self.ffn = nn.Sequential(\n",
        "            Linear(in_channels, in_channels * 4),\n",
        "            nn.ReLU(),\n",
        "            Linear(in_channels * 4, in_channels),\n",
        "        )\n",
        "        self.dropout = Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        x_flat = x.view(B, C, -1).permute(2, 0, 1)\n",
        "\n",
        "        attn_output, _ = self.multihead_attn(x_flat, x_flat, x_flat)\n",
        "        attn_output = attn_output.permute(1, 2, 0).view(B, C, H, W)\n",
        "\n",
        "        x = self.norm1(x + attn_output)\n",
        "        x = x + self.ffn(x.view(B, C, -1).permute(2, 0, 1)).permute(1, 2, 0).view(B, C, H, W)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ClassificationAndRegression(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ClassificationAndRegression, self).__init__()\n",
        "        self.cls_conv = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
        "        self.reg_conv = nn.Conv2d(in_channels, 4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_output = self.cls_conv(x)\n",
        "        reg_output = self.reg_conv(x)\n",
        "        return cls_output, reg_output\n",
        "\n",
        "class ModulationLayer(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super(ModulationLayer, self).__init__()\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.gap(x).view(b, c)\n",
        "        y = self.relu(self.fc1(y))\n",
        "        y = self.sigmoid(self.fc2(y)).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class HiFT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HiFT, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.feature_encoder = FeatureEncoder(in_channels=256, out_channels=256)\n",
        "        self.modulation_layer = ModulationLayer(in_channels=256)\n",
        "        self.feature_decoder = FeatureDecoder(in_channels=256)\n",
        "        self.classification_and_regression = ClassificationAndRegression(in_channels=256)\n",
        "        self.concat_conv = nn.Conv2d(256 * 2, 256, kernel_size=1)\n",
        "\n",
        "    def concatenate_and_conv(self, z, x):\n",
        "        concatenated = torch.cat((z, x), dim=1)\n",
        "        fused = self.concat_conv(concatenated)\n",
        "        return fused\n",
        "\n",
        "    def forward(self, z, x):\n",
        "        z_features = self.feature_extractor(z)\n",
        "        x_features = self.feature_extractor(x)\n",
        "\n",
        "        encoded_features = []\n",
        "        for i in range(3):\n",
        "            encoded = self.feature_encoder(z_features[i], x_features[i])\n",
        "            modulated = self.modulation_layer(encoded)\n",
        "            fused_features = self.concatenate_and_conv(z_features[i], x_features[i])\n",
        "            decoded = self.feature_decoder(fused_features)\n",
        "            encoded_features.append(decoded)\n",
        "\n",
        "        final_features = sum(encoded_features)\n",
        "        cls_output, reg_output = self.classification_and_regression(final_features)\n",
        "\n",
        "        return cls_output, reg_output\n"
      ],
      "metadata": {
        "id": "wA-hcEysUWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, dataloader, epochs, device):\n",
        "    model.train()\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_reg = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (template, search, cls_label, reg_label) in enumerate(dataloader):\n",
        "            template, search, cls_label, reg_label = template.to(device), search.to(device), cls_label.to(device), reg_label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            cls_output, reg_output = model(template, search)\n",
        "\n",
        "            cls_loss = criterion_cls(cls_output, cls_label)\n",
        "            reg_loss = criterion_reg(reg_output, reg_label)\n",
        "            loss = cls_loss + reg_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if i % 10 == 9:\n",
        "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 10:.3f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate model and move to device\n",
        "model = HiFT().to(device)\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "train(model, train_dataloader, epochs=10, device=device)\n"
      ],
      "metadata": {
        "id": "qSdPsUqcUWwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    criterion_reg = nn.MSELoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for template, search, cls_label, reg_label in dataloader:\n",
        "            template, search, cls_label, reg_label = template.to(device), search.to(device), cls_label.to(device), reg_label.to(device)\n",
        "\n",
        "            cls_output, reg_output = model(template, search)\n",
        "\n",
        "            cls_loss = criterion_cls(cls_output, cls_label)\n",
        "            reg_loss = criterion_reg(reg_output, reg_label)\n",
        "            loss = cls_loss + reg_loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Validation Loss: {avg_loss:.3f}\")\n",
        "\n",
        "# Validate the model\n",
        "validate(model, val_dataloader, device)\n"
      ],
      "metadata": {
        "id": "fr7JZklZUW0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}